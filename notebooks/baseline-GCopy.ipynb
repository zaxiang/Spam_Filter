{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbbc2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Garrett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Garrett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Garrett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\School\\DSC180\\Spam-Filter\\Spam_Filter\\Annotated\\insurance-etc\n",
      "E:\\School\\DSC180\\Spam-Filter\\Spam_Filter\\Annotated\\investment\n",
      "E:\\School\\DSC180\\Spam-Filter\\Spam_Filter\\Annotated\\medical-sales\n",
      "E:\\School\\DSC180\\Spam-Filter\\Spam_Filter\\Annotated\\phising\n",
      "E:\\School\\DSC180\\Spam-Filter\\Spam_Filter\\Annotated\\sexual\n",
      "E:\\School\\DSC180\\Spam-Filter\\Spam_Filter\\Annotated\\software-sales\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject  h  ello dea 54 r home owner  beetcn n...</td>\n",
       "      <td>insurance-etc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject make  171 hello  sent email ago  quali...</td>\n",
       "      <td>insurance-etc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject 96  refinance 2  9  hi would reflnance...</td>\n",
       "      <td>insurance-etc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject 82  refinance today  low 2  9  hey wou...</td>\n",
       "      <td>insurance-etc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject 6  refinance today premium low rate he...</td>\n",
       "      <td>insurance-etc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>subject  2   question soft mult gua msof  ilan...</td>\n",
       "      <td>software-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>subject  5   question progs mult gua msof  ila...</td>\n",
       "      <td>software-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>subject  7   talks soft mult gua msof  ilan ge...</td>\n",
       "      <td>software-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>subject dear sir  interested hi   need softwar...</td>\n",
       "      <td>software-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>__label__softwaresales subject coreldraw  100 ...</td>\n",
       "      <td>software-sales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2240 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence           label\n",
       "0     subject  h  ello dea 54 r home owner  beetcn n...   insurance-etc\n",
       "1     subject make  171 hello  sent email ago  quali...   insurance-etc\n",
       "2     subject 96  refinance 2  9  hi would reflnance...   insurance-etc\n",
       "3     subject 82  refinance today  low 2  9  hey wou...   insurance-etc\n",
       "4     subject 6  refinance today premium low rate he...   insurance-etc\n",
       "...                                                 ...             ...\n",
       "2235  subject  2   question soft mult gua msof  ilan...  software-sales\n",
       "2236  subject  5   question progs mult gua msof  ila...  software-sales\n",
       "2237  subject  7   talks soft mult gua msof  ilan ge...  software-sales\n",
       "2238  subject dear sir  interested hi   need softwar...  software-sales\n",
       "2239  __label__softwaresales subject coreldraw  100 ...  software-sales\n",
       "\n",
       "[2240 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import re\n",
    "def cleaning(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = re.sub(r'[^\\w\\s]', '', sentence.lower()).replace(\"\\n\", \" \").split(\" \")\n",
    "    cleaned = [token for token in tokens if token not in stop_words]\n",
    "    return \" \".join(cleaned)\n",
    "labels = [\"insurance-etc\",\"investment\", \"medical-sales\", \"phising\", \"sexual\", \"software-sales\"]\n",
    "text = []\n",
    "classes = []\n",
    "for label in labels:\n",
    "    path = os.getcwd()+\"\\\\Annotated\\\\\"+label\n",
    "    print(path)\n",
    "    os.chdir(path)\n",
    "    \n",
    "    def read_text_file(file_path):\n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "            return f.read()\n",
    "      \n",
    "    for file in os.listdir():\n",
    "    \n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = f\"{path}/{file}\"\n",
    "            text.append(cleaning(str(read_text_file(file_path))))\n",
    "            classes.append(label)\n",
    "    os.chdir(\"../..\")\n",
    "data = pd.DataFrame({'sentence':text, 'label':classes})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3444b1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(word):\n",
    "    sentence = data['sentence']\n",
    "    idf = np.log(len(sentence)/sentence.str.contains(word).sum())\n",
    "    result = []\n",
    "    for i in range(len(sentence)):\n",
    "        tf = sentence.iloc[i].count(word)/(len(sentence.iloc[i]))\n",
    "        result.append(tf*idf)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('seedwords.json')\n",
    "seeds = json.load(f)\n",
    "result = pd.DataFrame()\n",
    "for key, value in seeds.items():\n",
    "    df = pd.DataFrame()\n",
    "    for w in value:\n",
    "        df[w] = tfidf(w)\n",
    "    result[key] = df.sum(axis = 1)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c53b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"prediction\"] = result.idxmax(1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba42050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro and macro F1 using tf-idf\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013429c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.f1_score(data[\"label\"], data[\"prediction\"], average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f00df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(data[\"label\"], data[\"prediction\"], average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb39d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "def preprocessing(sentence):\n",
    "    tokens = sentence.split(\" \")\n",
    "    return [token for token in tokens if token!=\"\" and token != \" \"]\n",
    "features = data[\"sentence\"].apply(preprocessing)\n",
    "model = Word2Vec(sentences=features, vector_size=100, window=5, min_count=1, workers=4)\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ed801",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\")\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "model.train(features, total_examples=len(data), epochs=20)\n",
    "#vector = model.wv[\"atheism\"]\n",
    "#vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd44e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_per_label(filename):\n",
    "    f = open(filename)\n",
    "    seeds = json.load(f)\n",
    "    vector_per_label = []\n",
    "    for key, value in seeds.items():\n",
    "        lst = []\n",
    "        for w in value:\n",
    "            lst.append(model.wv[w])\n",
    "        arr = np.asarray(lst)\n",
    "        total = np.average(arr, axis=0)\n",
    "        vector_per_label.append(total)\n",
    "    return vector_per_label\n",
    "vector_per_label = get_vectors_per_label('seedwords.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18640f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7bcc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_per_doc(feature):\n",
    "    vector_per_doc = []\n",
    "    for feat in feature:\n",
    "        lst = []\n",
    "        for w in feat:\n",
    "            lst.append(model.wv[w])\n",
    "        arr = np.asarray(lst)\n",
    "        total = np.average(arr, axis=0)\n",
    "        vector_per_doc.append(total)\n",
    "    return vector_per_doc\n",
    "vector_per_doc = get_vector_per_doc(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89c5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97daeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('seedwords.json')\n",
    "seeds = json.load(f)\n",
    "from numpy.linalg import norm\n",
    "def predict_word2vec(vector_per_doc, vector_per_label):\n",
    "    predictions = []\n",
    "    labels = list(seeds.keys())\n",
    "    for doc in vector_per_doc:\n",
    "        cosine = []\n",
    "        for label in vector_per_label:\n",
    "            cosine.append(np.dot(doc,label)/(norm(doc)*norm(label)))\n",
    "        max_value = max(cosine)\n",
    "        max_index = cosine.index(max_value)\n",
    "        predictions.append(labels[max_index])\n",
    "    return predictions   \n",
    "prediction_word2vec = predict_word2vec(vector_per_doc, vector_per_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e804645",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"prediction_word2vec\"] = prediction_word2vec\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b4d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro and macro F1 using word2vec\n",
    "metrics.f1_score(data[\"label\"], data[\"prediction_word2vec\"], average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cbe228",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(data[\"label\"], data[\"prediction_word2vec\"], average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data[\"prediction_word2vec\"] == data[\"label\"])/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e789bc-4fa3-4b1b-b1a9-83ab25cb2c4a",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7559151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-62-cf238e56559e>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_set['pred'] = preds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>subject c _ _ _ l _ _ soft tabs hi  try revolu...</td>\n",
       "      <td>medical-sales</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>subject fw   canyon  71   vicodin  encamps nan...</td>\n",
       "      <td>medical-sales</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>subject congratulations goldland lotto interna...</td>\n",
       "      <td>phising</td>\n",
       "      <td>phising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>subject get pres  cription filled right  jq au...</td>\n",
       "      <td>medical-sales</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>subject  2   discussion health ge ri ia ne cc ...</td>\n",
       "      <td>medical-sales</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>subject purchase v  icodin online easily today...</td>\n",
       "      <td>medical-sales</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>subject percocet fed medlcat 10 n v  1  c  0  ...</td>\n",
       "      <td>medical-sales</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>subject top quality medication tribune soil ac...</td>\n",
       "      <td>medical-sales</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>subject tynenol 3 codeine legal cheap  rx  med...</td>\n",
       "      <td>medical-sales</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>subject mother pain pain tolerancy healing h  ...</td>\n",
       "      <td>medical-sales</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>896 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence          label  \\\n",
       "1413  subject c _ _ _ l _ _ soft tabs hi  try revolu...  medical-sales   \n",
       "680   subject fw   canyon  71   vicodin  encamps nan...  medical-sales   \n",
       "1484  subject congratulations goldland lotto interna...        phising   \n",
       "736   subject get pres  cription filled right  jq au...  medical-sales   \n",
       "1399  subject  2   discussion health ge ri ia ne cc ...  medical-sales   \n",
       "...                                                 ...            ...   \n",
       "788   subject purchase v  icodin online easily today...  medical-sales   \n",
       "1068  subject percocet fed medlcat 10 n v  1  c  0  ...  medical-sales   \n",
       "748   subject top quality medication tribune soil ac...  medical-sales   \n",
       "676   subject tynenol 3 codeine legal cheap  rx  med...  medical-sales   \n",
       "941   subject mother pain pain tolerancy healing h  ...  medical-sales   \n",
       "\n",
       "               pred  \n",
       "1413  medical-sales  \n",
       "680   medical-sales  \n",
       "1484        phising  \n",
       "736   medical-sales  \n",
       "1399  medical-sales  \n",
       "...             ...  \n",
       "788   medical-sales  \n",
       "1068  medical-sales  \n",
       "748   medical-sales  \n",
       "676   medical-sales  \n",
       "941   medical-sales  \n",
       "\n",
       "[896 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(data, test_size=0.4)\n",
    "with open('spam-train.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    for idx, row in train_set.iterrows():\n",
    "        f.write(\"__label__\" + row.label + \" \" + row.sentence + \"\\n\")\n",
    "model = fasttext.train_supervised(input='spam-train.txt', epoch=25, lr =0.5, wordNgrams=4, loss='hs', dim=50)\n",
    "fastText_df = pd.DataFrame(test_set.sentence)\n",
    "fastText_df['label'] = test_set.label\n",
    "preds = []\n",
    "for sentence in test_set.sentence:\n",
    "    pred = model.predict(sentence)\n",
    "    label = pred[0][0].replace(\"__label__\", \"\")\n",
    "    preds.append(label)\n",
    "test_set['pred'] = preds\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ad44ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8772321428571429"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(test_set[\"label\"], test_set[\"pred\"], average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad505e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7917269702653833"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(test_set[\"label\"], test_set[\"pred\"], average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ad0febf2-cc6c-428d-bb5a-213bfde7aed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>subject unleash animal  use ring  experience b...</td>\n",
       "      <td>sexual</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>subject scan system adware wrongful     613032...</td>\n",
       "      <td>software-sales</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>subject hp toronto  promo products dec 04 plea...</td>\n",
       "      <td>software-sales</td>\n",
       "      <td>insurance-etc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>subject hello dear member   welcome  happy rep...</td>\n",
       "      <td>sexual</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>subject anal 3 x please let image load    clos...</td>\n",
       "      <td>sexual</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>subject reliable source prescription drugs wit...</td>\n",
       "      <td>investment</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>subject   utf  8  q  young ang randy youn     ...</td>\n",
       "      <td>sexual</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>subject right   yet casualty anton prosodic ga...</td>\n",
       "      <td>insurance-etc</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>subject largest collection porn mo   ies ever ...</td>\n",
       "      <td>sexual</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>subject young sluts getting creamy facials you...</td>\n",
       "      <td>sexual</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence           label  \\\n",
       "1872  subject unleash animal  use ring  experience b...          sexual   \n",
       "1963  subject scan system adware wrongful     613032...  software-sales   \n",
       "2105  subject hp toronto  promo products dec 04 plea...  software-sales   \n",
       "1814  subject hello dear member   welcome  happy rep...          sexual   \n",
       "1816  subject anal 3 x please let image load    clos...          sexual   \n",
       "...                                                 ...             ...   \n",
       "352   subject reliable source prescription drugs wit...      investment   \n",
       "1856  subject   utf  8  q  young ang randy youn     ...          sexual   \n",
       "89    subject right   yet casualty anton prosodic ga...   insurance-etc   \n",
       "1895  subject largest collection porn mo   ies ever ...          sexual   \n",
       "1827  subject young sluts getting creamy facials you...          sexual   \n",
       "\n",
       "               pred  \n",
       "1872  medical-sales  \n",
       "1963  medical-sales  \n",
       "2105  insurance-etc  \n",
       "1814  medical-sales  \n",
       "1816  medical-sales  \n",
       "...             ...  \n",
       "352   medical-sales  \n",
       "1856  medical-sales  \n",
       "89    medical-sales  \n",
       "1895  medical-sales  \n",
       "1827  medical-sales  \n",
       "\n",
       "[110 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong = test_set[test_set['label'] != test_set['pred']]\n",
    "wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741801c-4b03-40d0-8798-17246f7d5745",
   "metadata": {},
   "source": [
    "## Test Unannotated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "703ced3b-f3df-4e7e-ac54-fc1be496b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_unannotated = []\n",
    "path = os.getcwd() + \"/enron6/spam\"\n",
    "os.chdir(path)\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{path}/{file}\"\n",
    "        text_unannotated.append(cleaning(str(read_text_file(file_path))))\n",
    "os.chdir(\"../..\")\n",
    "data_unannotated = pd.DataFrame({'sentence':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f08be47b-ff83-46b5-9ffc-791b5bf939b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject  h  ello dea 54 r home owner  beetcn n...</td>\n",
       "      <td>medical-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject make  171 hello  sent email ago  quali...</td>\n",
       "      <td>insurance-etc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject 96  refinance 2  9  hi would reflnance...</td>\n",
       "      <td>insurance-etc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject 82  refinance today  low 2  9  hey wou...</td>\n",
       "      <td>insurance-etc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject 6  refinance today premium low rate he...</td>\n",
       "      <td>insurance-etc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>subject  2   question soft mult gua msof  ilan...</td>\n",
       "      <td>software-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>subject  5   question progs mult gua msof  ila...</td>\n",
       "      <td>software-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>subject  7   talks soft mult gua msof  ilan ge...</td>\n",
       "      <td>software-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>subject dear sir  interested hi   need softwar...</td>\n",
       "      <td>software-sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>__label__softwaresales subject coreldraw  100 ...</td>\n",
       "      <td>phising</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2240 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence            pred\n",
       "0     subject  h  ello dea 54 r home owner  beetcn n...   medical-sales\n",
       "1     subject make  171 hello  sent email ago  quali...   insurance-etc\n",
       "2     subject 96  refinance 2  9  hi would reflnance...   insurance-etc\n",
       "3     subject 82  refinance today  low 2  9  hey wou...   insurance-etc\n",
       "4     subject 6  refinance today premium low rate he...   insurance-etc\n",
       "...                                                 ...             ...\n",
       "2235  subject  2   question soft mult gua msof  ilan...  software-sales\n",
       "2236  subject  5   question progs mult gua msof  ila...  software-sales\n",
       "2237  subject  7   talks soft mult gua msof  ilan ge...  software-sales\n",
       "2238  subject dear sir  interested hi   need softwar...  software-sales\n",
       "2239  __label__softwaresales subject coreldraw  100 ...         phising\n",
       "\n",
       "[2240 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "for sentence in data_unannotated.sentence:\n",
    "    pred = model.predict(sentence)\n",
    "    label = pred[0][0].replace(\"__label__\", \"\")\n",
    "    preds.append(label)\n",
    "data_unannotated['pred'] = preds\n",
    "data_unannotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a55f3943-b952-4e94-9e3b-5224a2b2cdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medical-sales     1156\n",
       "phising            352\n",
       "software-sales     303\n",
       "investment         173\n",
       "insurance-etc      164\n",
       "sexual              92\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_unannotated.pred.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775246b5-459b-4382-9c4c-86a169498304",
   "metadata": {},
   "source": [
    "## Exploring some of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f688427-6b56-43d3-987e-8c467d02da0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject request sadie shansel cater ivortex reticulum incommensurate antipode  assassinate enzyme fabricate cofactor shanty  vise sabdomen polar taken  elapse anthem functionary  curate atlantis pfedora contention avaricious forfeiture tclairvoyant cane jmagnolia lowland kajar  curlew  hawaiian quillwort  oval respected member  winner summer ra e  give way program  please inform since winner offer one time opportunity lower interest r te 3  99 percent  get prize coupon id  2518 thank  valerie mcnally promotion department antisemitic lissajous  converse omission decay fcircumscribe verity  superstitious wallis drier crosswalk  aeolian  ostracism salesgirl dependent crass  defrock  centipede onto gfloodlit dolomitic jdauphine grizzle upstand  agnomen vienna  backspace vanish irresolvable qbarricade redshank  danbury agriculture snifter turpuvdqt eternal  countenance broody arnold scarburetor priory  tmoroccan zgkvuow dog identify transmissible cathedral colloquia sunburnt fcabot franchise yrxprdbz claw  cooley floodlit cranny bedlam anteater daeaddi dexter  anvil euphrates  quaternary bellatrix citron  confidante  oelate ptowel melissa unary  deceitful  twofold  bodice cairo towhead corralled asteria chow rail kinglet boeotian canoga  uruguay cryptanalyze lahore composition proclivity goniometer ovate typographer sicily nasturtium deemphasize ijealousy accelerate sc hzombie centrifugate  confine wgypsy bookbind ypbselg wsubstitute baffin abase spectroscopic joy bicycle  wwzeetcr \n",
      "insurance-etc\n"
     ]
    }
   ],
   "source": [
    "print(data_unannotated.iloc[42].sentence)\n",
    "print(data_unannotated.iloc[42].pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "913fa293-d64a-426e-8b40-e6cfa5745bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject   utf  8  q  might become     utf  8  q  advantageo     utf  8  q  us chap gi     utf  8  q  rl    products used heal erectile dysfunction  well far  famed inability copulate  someone cannot procure  retain  inflexible vertical member suitable intimate action  drugs  appropriate used execution enhancer vantage tablets function two days fabricate physical structure click buy \n",
      "medical-sales\n"
     ]
    }
   ],
   "source": [
    "print(data_unannotated.iloc[1111].sentence)\n",
    "print(data_unannotated.iloc[1111].pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c2377b6-2ea7-4a4c-9a3e-ca656c21d3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject feel great time day summers 80  savings xanax  valium  phentermine  viagra email removal  go  alight impulse ingest impractical corrector beast postpone watershed audition midland stub entendre heinz fragile erickson barrel ymca clairenora provincial ridgepole absorb decelerate santa vertex decathlon posteriori dixon doherty wondrous cycad hawley airstrip especial cornucopia jugginghildebrand otherwise checksumming countersunk picasso laguerre mathematician cambrian invincible ballot brownian lactose nubia statuary cardiac sincere blanc sulfatebaleful eigenvector playwriting malarial nevins northward trickery blowfish impatient arrival cryptanalytic spearmint narbonne friar bathroom waistline cosine dioxidemcgrath wept altair elysee snook gardenia eclipse amorphous mendelevium bestubble honk allay escrow inertance peafowl\n",
      "medical-sales\n"
     ]
    }
   ],
   "source": [
    "print(data_unannotated.iloc[432].sentence)\n",
    "print(data_unannotated.iloc[432].pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1202cd91-0510-4f04-85e4-c3675b273857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject lupe come watch   lube neeeed  let glide priiiiide  make feeeeed  come riiiiiiide       address solute inconvertible earnest message earthmoving frontage alpenstock experiential ordinance graves bufflehead darpa mobcap rodeo puzzle crewman penitentiary brahms inbreed tadpole delouse reciprocal aristocratic militia pet wakeup pence gully holland capo  blacken cutoff middlebury ramify bellboy austere axon ferguson adolph affable scriptural dead neve closeup passage tombstone refer sud amazon circuit clapboard chipboard apperception ama pheasant acyclic canis marion nelsen wave debauch thoreau fabricate bronco bebop burgundian aliquot sari automaton  \n",
      "sexual\n"
     ]
    }
   ],
   "source": [
    "sexual = data_unannotated[data_unannotated['pred'] == 'sexual']\n",
    "print(sexual.iloc[12].sentence)\n",
    "print(sexual.iloc[12].pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddca6600-0eff-4bee-a128-42e2728d6b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject doctor contact  medicaldirectory  physiciansguid 7  000 hospitals  25  000 nursing homes 400  000 doctors united states health care database united states healthcare database comprehensive new product offered exclusively limited  time basis  complete database includes hospitals  hmo   group medical practices  nursing homes  physicians country  rapidly  changing industry  current healthcare information invaluable resource businesses organizations  united states healthcare database includes comprehensive information 7  000 hospitals  25  000 nursing homes 400  000 doctors mention hmos group medical practices  extensive reliable mailing list database key decision makers health care market  imagine increase marketing sales effectiveness made possible targeting key contacts name  reaching right decision maker critical success direct marketing campaigns  product  record indexed features name  address  phone fax  database available excel format cd rom  designed mailing lists merges  data selected state criteria type practice  used unlimited basis  introductory offer  cost completely new database  available exclusively cd  rom   575  00  reg   1  495   annual subscription  includes six editions  available  825  reg   2  995  order united states health care database  c   please complete information fax number 1  905  751  0199  tel  905  751  0919     would like order united states health care database   575   please invoice    would like annual subscription  six editions  825   please invoice  name  title  organization  address  city  postal  tel  fax  e  mail  order united states health care database  c   please complete information fax 1  905  751  0199  tel  1  905  751  0919  infosource group companies leading information publishing firm offices throughout north america europe  removed database please follow link  http    notinuse  biz  takeoff  takeoff  html \n",
      "software-sales\n"
     ]
    }
   ],
   "source": [
    "software = data_unannotated[data_unannotated['pred'] == 'software-sales']\n",
    "print(software.iloc[9].sentence)\n",
    "print(software.iloc[9].pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "028e51ed-d6be-4dff-a37d-f8eae1ccaa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject stock profiier belanger  continue  important  expected  u c p  wi   large pr campaign next 10 days positive news expected  watch  jump board whiie stock beiow  1  huge promo weekend expected expect soar monday  tuesday next week  jump today  voice internet protoco   voip  service goes live symbol   u c p  current price   0  28 10 days target price   1  25 3 months target price   1  66  u c p  currentiy trading  0  28 headed  1  25 company reieased ground breaking news voip division   aithough would argue voip stil  maturing  corporate users extremeiy interested impiementing technoiogy  creating exponential growth  within last four years  voip minutes increased  ess 0  5 2 percent outbound international calis  according research telegeography  additionally  predictions size market vary    ied business inte   igence projecting voip market grow  3  7 bi   ion 2000  12  3 billion 2006  synergy research projecting voip equipment market grow  13  3 biliion 2005  uauthorize corporation e  business hoiding company builds acquires multiple websites  software titles   e  commerce soiutions  everage internet maximize success e  business operations  uauthorize also results  oriented marketer technology products  services  comprehensive portfoiio products  services  uauthorize attracts highly qualified audience technology product service buyers  company  successful business model based muitipie growth drivers  inciuding growth technology products service  cross  seliing additiona  products new affiliate signings  profile  uauthorize corp symbol   u c p  current price   0  28 rating  undervaiued beiieve speculative near term target price   1  25 believe speculative long term target price   1  50 voice internet protocol  voip  service goes live beiieve best pick since march 2004   real company real products stock headed                information within email contains  forward looking statements  within meaning section 27 securities act 1933 section 21 b securities exchange act 1934  statements express involve discussions respect predictions  goals  expectations  beliefs  plans  projections  objectives  assumptions future events performance statements historical fact may  forward looking statements   forward looking statements based expectations  estimates projections time statements made involve number risks uncertainties could cause actual results events differ materially presently anticipated  forward looking statements action may identified use words   projects    foresee    expects    estimates    believes    understands      part   anticipates   statements indicating certain actions  may    could    might  occur  information provided within email pertaining investing  stocks  securities must understood information provided investment advice  advise readers subscribers seek advice registered professional securities representative deciding trade stocks featured within email  none material within report shall construed kind investment advice  please mind interpretation witer newsletter news published company represent company official statement fact may differ real meaning news release meant say  ple  se  news release judge details  compliance section 17  b   disclose holding u c p shares prior publication report  aware inherent conflict interest resulting holdings due intent profit liquidation shares  shares may sold time  even positive statements made regarding company  since shares  inherent conflict interest statements opinions  readers publication cautioned place undue reliance forward  looking statements  based certain assumptions expectations involving various risks uncertainties  could cause results differ materially set forth forward  looking statements  please advised nothing within email shall constitute solicitation offer b u sell security mentioned herein  newsletter neither registered investment advisor affiliated broker dealer  statements made express opinion treated  may  b u sell securities mentioned time  report includes forward  looking statements within meaning private securities litigation reform act 1995  statements may include terms  expect    believe    may       move    undervalued   intend  similar terms  newsletter paid  12500 third party send report  please due diligence investing profiled company  may lose money investing penny stocks \n",
      "investment\n"
     ]
    }
   ],
   "source": [
    "investment = data_unannotated[data_unannotated['pred'] == 'investment']\n",
    "print(investment.iloc[29].sentence)\n",
    "print(investment.iloc[29].pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "642f3784-30a6-40d1-be57-3c9a0eb7cacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject winning notification  lottery coordinator  international promotions  prize award department dear winner  results category   draws congratulations bring notice  results first category draws lucky strike lottery uk  happy inform emerged winner first category  part promotional draws  draws held day prior notification results officially announced  participants selected computer ballot system drawn 2  500  000 names  email addresses individuals companies africa  america  asia  australia  europe  middle east  oceania part international promotions program   company  attached ticket number 6422  5  486  serial number 59  18 drew lucky numbers 33  92  78  05  18  consequently first category  therefore awarded lump sum pay  6  500  000  six million  five hundred thousand great britain pounds   winning payout category winners  total prize money  13  000  000 shared among 2 winners category  congratulations  fund deposited transfer agents cash change uk ltd insured name  best interest also avoid mix numbers names kind  request keep entire details award strictly public notice process transferring claims completed  funds remitted account  part security protocol avoid double claiming unscrupulous acts participants  non participants program  also wish bring notice end year premium stakes draw stand chance winning  50 million  hope part prize participate  please contact claims agent immediately due processing remittance prize money designated account choice  mr  andrew young  foreign department manager  cash change uk ltd foreign service number   88216 4665 5376 tel   44 2070600695 fax   44 2079002649 advised contact agents email fax within week receiving notice  failure may warrant disqualification  note  easy reference identification  find reference batch numbers  remember quote numbers every one correspondences claims agent  reference number  lsluk  2031  8161  04 batch number  r 3  312  59 congratulations staff thank part promotions program  sincerely  lottery coordinator  lucky strike lottery uk 12 bridge street  staines middlesex twl 8 4 tp uk n  b  breach confidentiality part winners result disqualification  please contact claims agent immediately  mail sent webmail service php  nuke powered site  http    www  teamnitroassassins  com\n",
      "phising\n"
     ]
    }
   ],
   "source": [
    "phising = data_unannotated[data_unannotated['pred'] == 'phising']\n",
    "print(phising.iloc[21].sentence)\n",
    "print(phising.iloc[21].pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c21451e-a617-41c2-864a-130091c8ae52",
   "metadata": {},
   "source": [
    "## Seems good at predicting these unannotated files, but is it still overfitting to only this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "181bcbc3-b1d7-43dc-9f03-a72643b946cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__software-sales',\n",
       "  '__label__insurance-etc',\n",
       "  '__label__medical-sales',\n",
       "  '__label__phising',\n",
       "  '__label__insuranceetc'),\n",
       " array([0.47605869, 0.47433549, 0.02369787, 0.0191186 , 0.00666779]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"congrats you have won 1 million usd in the lottery from usa. please click here to earn your prize\", k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c8090c-8310-47ed-b3b8-1f245e71d926",
   "metadata": {},
   "source": [
    "Wrong label, phising is all the way at the 4th contender!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98f384a2-bb0a-470a-aeef-90cc392d99d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__medical-sales',\n",
       "  '__label__sexual',\n",
       "  '__label__medicalsales',\n",
       "  '__label__software-sales',\n",
       "  '__label__insurance-etc'),\n",
       " array([0.77838385, 0.1330519 , 0.03946874, 0.02377219, 0.01588757]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"want to watch webcams of young teen girls to get your affair on today in bed\", k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d140089b-498a-44b7-a3aa-15799316a539",
   "metadata": {},
   "source": [
    "Another wrong label, very confident this is medical???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c96148-2bf2-413e-a933-124e8cea0a3d",
   "metadata": {},
   "source": [
    "## Work on spam/not spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "70c85800-f09a-41fc-b966-13364cfacfe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject key dates impact upcoming sap implemen...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject transportation resort please informed ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject human resources organization enron con...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject want know today   man new idea crank s...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject tw weekly  6  9  00 please see attache...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>subject tw weekend scheduled volumes march 200...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>subject fw  ivanhoe e   fyi  kim       origina...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>subject fw  abandoned pipe ownership fyi  kim ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>subject fw  tw question amarillo fyi  kim     ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>subject fw  ivanhoe e   fyi  kim       origina...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence label\n",
       "0     subject key dates impact upcoming sap implemen...   ham\n",
       "1     subject transportation resort please informed ...   ham\n",
       "2     subject human resources organization enron con...   ham\n",
       "3     subject want know today   man new idea crank s...   ham\n",
       "4     subject tw weekly  6  9  00 please see attache...   ham\n",
       "...                                                 ...   ...\n",
       "1495  subject tw weekend scheduled volumes march 200...   ham\n",
       "1496  subject fw  ivanhoe e   fyi  kim       origina...   ham\n",
       "1497  subject fw  abandoned pipe ownership fyi  kim ...   ham\n",
       "1498  subject fw  tw question amarillo fyi  kim     ...   ham\n",
       "1499  subject fw  ivanhoe e   fyi  kim       origina...   ham\n",
       "\n",
       "[1500 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonspam = []\n",
    "path = os.getcwd() + \"/enron6/ham\"\n",
    "os.chdir(path)\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{path}/{file}\"\n",
    "        #print(file_path)\n",
    "        nonspam.append(cleaning(str(read_text_file(file_path))))\n",
    "os.chdir(\"../..\")\n",
    "nonspam = pd.DataFrame({'sentence':nonspam})\n",
    "nonspam['label'] = \"ham\"\n",
    "nonspam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3d49e709-0736-4f1e-957d-dace1038d2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subject gss organizational changes pleased announce following additions enron global strategic sourcing operations group  effective june 1  tracy ramsey joined group sourcing portfolio leader cheryl slone joined group travel coordinator  new position  tracy responsible overseeing travel  entertainment conference  related sourcing activities  held various positions within travel industry recently served manager corporate travel enron property services corp  tracy report directly  cheryl joined enron 1998 travel coordinator  brings seven years travel industry experience new position  prior joining enron  recently worked travel management services   anderson cancer center  additionally  amanda becher joined group supply analyst  prior joining enron  held various positions inventory planning analysis  recently  served senior customer inventory analyst w  w  grainger  inc  cheryl holds b  b   operations management university houston  please join welcoming tracy  cheryl amanda group '"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonspam.sentence.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1ac655d3-0793-445a-96f4-37ab511c2c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject nasýnsýn ortaksat  07 aug 2004 23  08 ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject f 0 r b 1 e n â â â l 0 l 1 â â â drea...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject carry bawek arly buczek</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject 3  jpg joke day blonde went appliance ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject today hey sheppard   heard awhile want...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>subject  5   question progs mult gua msof  ila...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>subject  7   talks soft mult gua msof  ilan ge...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>subject everything need beautiful hardwood flo...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>subject reduzca sus gastos telefonicos empresa...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>subject make big bucks medical field bait  exc...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2672 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence label\n",
       "0     subject nasýnsýn ortaksat  07 aug 2004 23  08 ...  spam\n",
       "1     subject f 0 r b 1 e n â â â l 0 l 1 â â â drea...  spam\n",
       "2                      subject carry bawek arly buczek   spam\n",
       "3     subject 3  jpg joke day blonde went appliance ...  spam\n",
       "4     subject today hey sheppard   heard awhile want...  spam\n",
       "...                                                 ...   ...\n",
       "2667  subject  5   question progs mult gua msof  ila...  spam\n",
       "2668  subject  7   talks soft mult gua msof  ilan ge...  spam\n",
       "2669  subject everything need beautiful hardwood flo...  spam\n",
       "2670  subject reduzca sus gastos telefonicos empresa...  spam\n",
       "2671  subject make big bucks medical field bait  exc...  spam\n",
       "\n",
       "[2672 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = []\n",
    "path = os.getcwd() + \"/enron6/spam\"\n",
    "os.chdir(path)\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{path}/{file}\"\n",
    "        spam.append(cleaning(str(read_text_file(file_path))))\n",
    "os.chdir(\"../..\")\n",
    "spam = pd.DataFrame({'sentence':spam})\n",
    "spam['label'] = \"spam\"\n",
    "spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff21d2-19bd-4bfb-a268-ff15cce818de",
   "metadata": {},
   "source": [
    "## Try FastText with spam/nonspam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79ab51fd-f7d6-44b1-8ca9-1ec183d3c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.concat([nonspam, spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "917fda7f-8e3a-4949-bed0-a08c6c088378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-98-cc2d4ca4d012>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_set['pred'] = preds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>subject enron bids farewell hpl enron sold hou...</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>subject learn get freedom    freedom choice pr...</td>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>subject weekly throughput report week october ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>subject california brink   cera alert         ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>subject etc  event  schlitterbahn good news   ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>subject urgent  mr  mandisi bongani mabuto e  ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>subject   utf  8  q   rolex order deta     utf...</td>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>subject enron action 09  25  00 chairman  awar...</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>subject save us rafael swiss pharmacy online w...</td>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>subject power outage outages  free information...</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1669 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence label  pred\n",
       "669   subject enron bids farewell hpl enron sold hou...   ham   ham\n",
       "916   subject learn get freedom    freedom choice pr...  spam  spam\n",
       "424   subject weekly throughput report week october ...   ham   ham\n",
       "569   subject california brink   cera alert         ...   ham   ham\n",
       "1017  subject etc  event  schlitterbahn good news   ...   ham   ham\n",
       "...                                                 ...   ...   ...\n",
       "144   subject urgent  mr  mandisi bongani mabuto e  ...  spam  spam\n",
       "233   subject   utf  8  q   rolex order deta     utf...  spam  spam\n",
       "296   subject enron action 09  25  00 chairman  awar...   ham   ham\n",
       "1557  subject save us rafael swiss pharmacy online w...  spam  spam\n",
       "1391  subject power outage outages  free information...   ham   ham\n",
       "\n",
       "[1669 rows x 3 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(dat, test_size=0.4)\n",
    "with open('spamham-train.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    for idx, row in train_set.iterrows():\n",
    "        f.write(\"__label__\" + row.label +  \" \" + row.sentence + \"\\n\")\n",
    "model = fasttext.train_supervised(input='spamham-train.txt', epoch=25, lr =0.1, wordNgrams=2, loss='hs', dim=50)\n",
    "preds = []\n",
    "for sentence in test_set.sentence:\n",
    "    pred = model.predict(sentence)\n",
    "    label = pred[0][0].replace(\"__label__\", \"\")\n",
    "    preds.append(label)\n",
    "test_set['pred'] = preds\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2999dd88-d07e-4cea-8a60-c5b0a8e5600d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam    1091\n",
       "ham      578\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "26d237ea-50cc-4961-9bbd-a82ec2f9dd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9784301977231875"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(test_set[\"label\"], test_set[\"pred\"], average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bc277add-8467-43aa-83ac-5b763f85accf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9764586219650571"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(test_set[\"label\"], test_set[\"pred\"], average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc1e05c8-2bf2-416b-8996-1bb6e3fa7812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__spam', '__label__ham'), array([0.6627382 , 0.33728182]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"Don't forget the work meeting on 5/12 for our employers! We are looking forward to having our vendors at this meeting.\", k = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
