{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "77aebd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xiangzairan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/xiangzairan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/xiangzairan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xiangzairan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/xiangzairan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/xiangzairan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import norm\n",
    "from sklearn import metrics\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# from src.features.features import *\n",
    "# from src.data.generate_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1fbf0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaning(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = re.sub(r'[^\\w\\s]', '', sentence.lower()).replace(\"\\n\", \" \").split(\" \")\n",
    "    cleaned = [token for token in tokens if token not in stop_words]\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    tokens = sentence.split(\" \")\n",
    "    return [token for token in tokens if token!=\"\" and token != \" \"]\n",
    "\n",
    "#combination version of both preprocessing and clearning \n",
    "#this turn out to be so bad - <0.5 accuracy for annotated training&testing; and 0.1-0.2 for unannotated training\n",
    "def new_preprocessing(sentence):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punct = string.punctuation\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    doc = str(sentence.lower())\n",
    "    doc = [i for i in doc if not (i in punct)] # non-punct characters\n",
    "    doc = ''.join(doc)\n",
    "    tokens = re.sub(r'[^\\w\\s]', '', sentence.lower()).replace(\"\\n\", \" \").split(\" \") #tokenizer\n",
    "    tokens = [t for t in tokens if t not in stop_words] #remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(stemmer.stem(t)) for t in tokens] #stemmer and lemmatizer\n",
    "    tokens = [t for t in tokens if t!=\"\" and t != \" \"]\n",
    "    return \" \".join(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aec7c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data):\n",
    "    features = data['sentence'].apply(preprocessing) \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8f7140d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_data(target):\n",
    "    #training on all unannotated data, and text on the annotated\n",
    "    if target == 'full':\n",
    "        text = []\n",
    "        path = \"data/raw/spam/Unannotated/\"\n",
    "        all_files = os.listdir(path)\n",
    "        for fil in all_files:\n",
    "            if fil.endswith(\".txt\"):\n",
    "                file_path = path + \"/\" + fil\n",
    "                with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "                    text.append(cleaning(str(f.read())))\n",
    "        return pd.DataFrame({'sentence':text})\n",
    "\n",
    "    else:\n",
    "        labels = [\"insurance-etc\",\"investment\", \"medical-sales\", \"phising\", \"sexual\", \"software-sales\"]\n",
    "        text = []\n",
    "        classes = []\n",
    "        for cla in labels:\n",
    "            path = \"data/raw/spam/Annotated/\"\n",
    "            if target == 'test':\n",
    "                path = \"test/testdata/\"\n",
    "            all_files = os.listdir(path + cla)\n",
    "            for fil in all_files:\n",
    "                if fil.endswith(\".txt\"):\n",
    "                    file_path = path + cla + \"/\" + fil\n",
    "                    with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "                        text.append(cleaning(str(f.read())))\n",
    "                        classes.append(cla)\n",
    "        return pd.DataFrame({'sentence':text, 'label':classes})\n",
    "\n",
    "\n",
    "\n",
    "def get_annotated_data():\n",
    "    labels = [\"insurance-etc\",\"investment\", \"medical-sales\", \"phising\", \"sexual\", \"software-sales\"]\n",
    "    text = []\n",
    "    classes = []\n",
    "    for cla in labels:\n",
    "        path = \"data/raw/spam/Annotated/\"\n",
    "        all_files = os.listdir(path + cla)\n",
    "        for fil in all_files:\n",
    "            if fil.endswith(\".txt\"):\n",
    "                file_path = path + cla + \"/\" + fil\n",
    "                with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "                    text.append(cleaning(str(f.read())))\n",
    "                    classes.append(cla)\n",
    "    return pd.DataFrame({'sentence':text, 'label':classes})\n",
    "\n",
    "\n",
    "\n",
    "def generate_fasttext_train_data(train_set):\n",
    "    with open('data/out/spam-train.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        for idx, row in train_set.iterrows():\n",
    "            f.write(row.sentence + \"\\n\")\n",
    "            \n",
    "#     with open('data/out/spam-train.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#         for idx, row in train_set.iterrows():\n",
    "#             f.write(\"__label__\" + row.label + \" \" + row.sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d2588cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_per_label(filename, model):\n",
    "    f = open(filename)\n",
    "    seeds = json.load(f)\n",
    "    vector_per_label = []\n",
    "    for key, value in seeds.items():\n",
    "        lst = []\n",
    "        for w in value:\n",
    "            lst.append(model.get_word_vector(w))\n",
    "        arr = np.asarray(lst)\n",
    "        total = np.average(arr, axis=0)\n",
    "        vector_per_label.append(total)\n",
    "    return vector_per_label\n",
    "\n",
    "def get_vector_per_doc(feature, model):\n",
    "    vector_per_doc = []\n",
    "    for feat in feature:\n",
    "        lst = []\n",
    "        for w in feat:\n",
    "            lst.append(model.get_word_vector(w))\n",
    "        arr = np.asarray(lst)\n",
    "        total = np.average(arr, axis=0)\n",
    "        vector_per_doc.append(total)\n",
    "    return vector_per_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d833557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FastText(target):\n",
    "    if target == \"data\" or target == 'test':\n",
    "        data = read_data(target)\n",
    "        train_set, test_set = train_test_split(data, test_size=0.4)\n",
    "    \n",
    "    #training on all unannotated data, and text on the annotated\n",
    "    if target == \"full\":\n",
    "        train_set = read_data(target) #2672 \n",
    "        test_set = get_annotated_data() #2342 \n",
    "    \n",
    "    generate_fasttext_train_data(train_set) #write to txt and train on the unannotated data\n",
    "    model = fasttext.train_unsupervised(input='data/out/spam-train.txt', epoch=600, lr=0.05, wordNgrams=4, loss='hs', dim=40)\n",
    "\n",
    "    filename = 'data/out/seedwords.json'\n",
    "    vec_per_label = get_vectors_per_label(filename, model)\n",
    "    features = get_features(test_set) #new pre-processing on test data df\n",
    "    vec_per_doc = get_vector_per_doc(features, model) #vector should be test data vector (annotated)\n",
    "    labels = test_set['label']\n",
    "    pred = predict(vec_per_doc, vec_per_label, filename)\n",
    "    return labels, pred\n",
    "\n",
    "\n",
    "def predict(vector_per_doc, vector_per_label, filename):\n",
    "    predictions = []\n",
    "    f = open(filename)\n",
    "    seeds = json.load(f)\n",
    "    labels = list(seeds.keys())\n",
    "    for doc in vector_per_doc:\n",
    "        cosine = []\n",
    "        for label in vector_per_label:\n",
    "            cosine.append(np.dot(doc,label)/(norm(doc)*norm(label)))\n",
    "        max_value = max(cosine)\n",
    "        max_index = cosine.index(max_value)\n",
    "        predictions.append(labels[max_index])\n",
    "    return predictions   \n",
    "\n",
    "\n",
    "def get_accuracy(pred, label):\n",
    "    micro = metrics.f1_score(label, pred, average=\"micro\")\n",
    "    macro = metrics.f1_score(label, pred, average=\"macro\")\n",
    "    return micro, macro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1b17f",
   "metadata": {},
   "source": [
    "### I am manually training and getting prediction below, instead of using the above three func; calling the func would output the same thing\n",
    "\n",
    "- training on unannotated\n",
    "- testing on annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c38772cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "86f4f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_fasttext_train_data(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ed6af73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  8566\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   71153 lr:  0.000000 avg.loss:  1.747089 ETA:   0h 0m 0s  6.0% words/sec/thread:   82651 lr:  0.047024 avg.loss:  6.483490 ETA:   0h 6m50s  6.5% words/sec/thread:   82789 lr:  0.046738 avg.loss:  6.477500 ETA:   0h 6m47s 15.8% words/sec/thread:   79453 lr:  0.042086 avg.loss:  6.117484 ETA:   0h 6m21s 16.5% words/sec/thread:   79405 lr:  0.041730 avg.loss:  6.012546 ETA:   0h 6m18s 17.0% words/sec/thread:   79365 lr:  0.041520 avg.loss:  5.952402 ETA:   0h 6m17s 18.9% words/sec/thread:   79318 lr:  0.040553 avg.loss:  5.717841 ETA:   0h 6m 8s 20.0% words/sec/thread:   79271 lr:  0.040007 avg.loss:  5.597327 ETA:   0h 6m 3s 23.6% words/sec/thread:   76792 lr:  0.038194 avg.loss:  5.245925 ETA:   0h 5m58s 27.0% words/sec/thread:   74126 lr:  0.036524 avg.loss:  4.982929 ETA:   0h 5m55s 31.7% words/sec/thread:   73168 lr:  0.034143 avg.loss:  4.631298 ETA:   0h 5m36s 34.3% words/sec/thread:   72985 lr:  0.032847 avg.loss:  4.512743 ETA:   0h 5m24s 36.2% words/sec/thread:   72807 lr:  0.031909 avg.loss:  4.418363 ETA:   0h 5m16s 36.7% words/sec/thread:   72773 lr:  0.031628 avg.loss:  4.392720 ETA:   0h 5m13s 36.8% words/sec/thread:   72772 lr:  0.031608 avg.loss:  4.390837 ETA:   0h 5m13s% words/sec/thread:   72723 lr:  0.031310 avg.loss:  4.362889 ETA:   0h 5m10s% words/sec/thread:   72694 lr:  0.031049 avg.loss:  4.353648 ETA:   0h 5m 7s 42.2% words/sec/thread:   72558 lr:  0.028918 avg.loss:  4.076727 ETA:   0h 4m47s 43.2% words/sec/thread:   72511 lr:  0.028377 avg.loss:  3.975351 ETA:   0h 4m42s 45.4% words/sec/thread:   72406 lr:  0.027317 avg.loss:  3.791776 ETA:   0h 4m32s 45.4% words/sec/thread:   72403 lr:  0.027297 avg.loss:  3.788486 ETA:   0h 4m31s 47.9% words/sec/thread:   72183 lr:  0.026063 avg.loss:  3.595420 ETA:   0h 4m20s 48.0% words/sec/thread:   72173 lr:  0.025983 avg.loss:  3.583711 ETA:   0h 4m19s 53.9% words/sec/thread:   72013 lr:  0.023026 avg.loss:  3.194858 ETA:   0h 3m50s 54.3% words/sec/thread:   72007 lr:  0.022834 avg.loss:  3.173542 ETA:   0h 3m48s 57.2% words/sec/thread:   71960 lr:  0.021388 avg.loss:  3.015458 ETA:   0h 3m34s 60.1% words/sec/thread:   71833 lr:  0.019939 avg.loss:  2.871912 ETA:   0h 3m20s 63.0% words/sec/thread:   71757 lr:  0.018504 avg.loss:  2.743973 ETA:   0h 3m 5s 63.2% words/sec/thread:   71759 lr:  0.018391 avg.loss:  2.734273 ETA:   0h 3m 4s 64.6% words/sec/thread:   71742 lr:  0.017725 avg.loss:  2.678568 ETA:   0h 2m58s 72.2% words/sec/thread:   71588 lr:  0.013918 avg.loss:  2.400100 ETA:   0h 2m20s 73.1% words/sec/thread:   71560 lr:  0.013443 avg.loss:  2.369511 ETA:   0h 2m15s 73.3% words/sec/thread:   71478 lr:  0.013353 avg.loss:  2.363865 ETA:   0h 2m14s 73.9% words/sec/thread:   71468 lr:  0.013073 avg.loss:  2.346159 ETA:   0h 2m11s 77.7% words/sec/thread:   71403 lr:  0.011135 avg.loss:  2.231459 ETA:   0h 1m52s 78.8% words/sec/thread:   71393 lr:  0.010613 avg.loss:  2.202566 ETA:   0h 1m47s 78.8% words/sec/thread:   71393 lr:  0.010593 avg.loss:  2.201527 ETA:   0h 1m46s 79.2% words/sec/thread:   71387 lr:  0.010404 avg.loss:  2.191138 ETA:   0h 1m45s 79.4% words/sec/thread:   71382 lr:  0.010286 avg.loss:  2.184736 ETA:   0h 1m43s 79.5% words/sec/thread:   71382 lr:  0.010266 avg.loss:  2.183704 ETA:   0h 1m43s 79.6% words/sec/thread:   71384 lr:  0.010184 avg.loss:  2.179308 ETA:   0h 1m42s 82.2% words/sec/thread:   71278 lr:  0.008906 avg.loss:  2.113083 ETA:   0h 1m30s 83.7% words/sec/thread:   71280 lr:  0.008127 avg.loss:  2.074694 ETA:   0h 1m22s 85.6% words/sec/thread:   71257 lr:  0.007187 avg.loss:  2.030334 ETA:   0h 1m12s 86.1% words/sec/thread:   71255 lr:  0.006926 avg.loss:  2.018484 ETA:   0h 1m10s 91.6% words/sec/thread:   71203 lr:  0.004210 avg.loss:  1.902475 ETA:   0h 0m42s 91.7% words/sec/thread:   71204 lr:  0.004169 avg.loss:  1.900810 ETA:   0h 0m42s 93.2% words/sec/thread:   71212 lr:  0.003384 avg.loss:  1.869806 ETA:   0h 0m34s 94.5% words/sec/thread:   71175 lr:  0.002771 avg.loss:  1.846299 ETA:   0h 0m28s 97.3% words/sec/thread:   71159 lr:  0.001332 avg.loss:  1.793527 ETA:   0h 0m13s100.0% words/sec/thread:   71166 lr:  0.000001 avg.loss:  1.747162 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_unsupervised(input='data/out/spam-train.txt', epoch=600, lr=0.05, wordNgrams=4, loss='hs', dim=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e1cd03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed words\n",
    "filename = 'data/out/seedwords.json'\n",
    "vec_per_label = get_vectors_per_label(filename, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fd2eab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing data\n",
    "test_set = get_annotated_data()\n",
    "features = get_features(test_set) #new pre-processing on test data df\n",
    "vec_per_doc = get_vector_per_doc(features, model) #vector should be test data vector (annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c2a4c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get prediction and true label\n",
    "labels = test_set['label']\n",
    "pred = predict(vec_per_doc, vec_per_label, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "17dbdfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2342"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#old_pred = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c526f",
   "metadata": {},
   "source": [
    "#### Accuracy Score when training on Unannotated, and testing on Annotated (so the model never saw the testing set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d2990bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 = 0.6208368915456874\n",
      "Macro F1 = 0.6826916987592755\n"
     ]
    }
   ],
   "source": [
    "#getting accuracy score on the testing (annotated data)\n",
    "micro, macro = get_accuracy(pred, labels)\n",
    "print('Micro F1 = ' + str(micro))\n",
    "print('Macro F1 = ' + str(macro))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b21a7",
   "metadata": {},
   "source": [
    "#### Accuracy Score when training and testing on the same Annotated dataset, so the model already seen the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6d867add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  4950\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   80019 lr:  0.000000 avg.loss:  3.658798 ETA:   0h 0m 0s  2.1% words/sec/thread:  102090 lr:  0.048935 avg.loss:  6.244893 ETA:   0h 2m49s  3.5% words/sec/thread:  102718 lr:  0.048274 avg.loss:  6.214023 ETA:   0h 2m46s 13.8% words/sec/thread:   89853 lr:  0.043086 avg.loss:  6.129863 ETA:   0h 2m49s 16.1% words/sec/thread:   88459 lr:  0.041966 avg.loss:  6.168910 ETA:   0h 2m47s 16.5% words/sec/thread:   88409 lr:  0.041765 avg.loss:  6.181667 ETA:   0h 2m46s 23.5% words/sec/thread:   87984 lr:  0.038260 avg.loss:  6.302581 ETA:   0h 2m33s 26.3% words/sec/thread:   87884 lr:  0.036849 avg.loss:  6.335758 ETA:   0h 2m28s 33.1% words/sec/thread:   87329 lr:  0.033438 avg.loss:  5.938246 ETA:   0h 2m15s 34.7% words/sec/thread:   87241 lr:  0.032646 avg.loss:  5.791932 ETA:   0h 2m12s 35.7% words/sec/thread:   86993 lr:  0.032143 avg.loss:  5.708642 ETA:   0h 2m10s 36.6% words/sec/thread:   86944 lr:  0.031724 avg.loss:  5.647030 ETA:   0h 2m 8s 37.1% words/sec/thread:   86899 lr:  0.031434 avg.loss:  5.607671 ETA:   0h 2m 7s 38.2% words/sec/thread:   86897 lr:  0.030882 avg.loss:  5.542729 ETA:   0h 2m 5s 40.4% words/sec/thread:   86885 lr:  0.029798 avg.loss:  5.415677 ETA:   0h 2m 1s 50.3% words/sec/thread:   85874 lr:  0.024841 avg.loss:  4.945260 ETA:   0h 1m42s 50.4% words/sec/thread:   85850 lr:  0.024798 avg.loss:  4.942458 ETA:   0h 1m42s 50.7% words/sec/thread:   85711 lr:  0.024640 avg.loss:  4.931038 ETA:   0h 1m41s 51.5% words/sec/thread:   85428 lr:  0.024227 avg.loss:  4.901854 ETA:   0h 1m40s 52.5% words/sec/thread:   85099 lr:  0.023759 avg.loss:  4.864387 ETA:   0h 1m38s 56.7% words/sec/thread:   83867 lr:  0.021651 avg.loss:  4.728318 ETA:   0h 1m31s 62.0% words/sec/thread:   83243 lr:  0.019018 avg.loss:  4.578959 ETA:   0h 1m20s 63.3% words/sec/thread:   83150 lr:  0.018353 avg.loss:  4.543231 ETA:   0h 1m18s 65.7% words/sec/thread:   82986 lr:  0.017139 avg.loss:  4.486576 ETA:   0h 1m12s  82902 lr:  0.016670 avg.loss:  4.465899 ETA:   0h 1m11s 67.0% words/sec/thread:   82872 lr:  0.016490 avg.loss:  4.458202 ETA:   0h 1m10s 69.0% words/sec/thread:   82436 lr:  0.015498 avg.loss:  4.408241 ETA:   0h 1m 6s 74.8% words/sec/thread:   81779 lr:  0.012604 avg.loss:  4.288661 ETA:   0h 0m54s 79.5% words/sec/thread:   81458 lr:  0.010227 avg.loss:  4.199109 ETA:   0h 0m44s 81.2% words/sec/thread:   81413 lr:  0.009379 avg.loss:  4.166677 ETA:   0h 0m40s 83.5% words/sec/thread:   81369 lr:  0.008270 avg.loss:  4.134964 ETA:   0h 0m35s 86.6% words/sec/thread:   81171 lr:  0.006705 avg.loss:  4.083421 ETA:   0h 0m29s 87.9% words/sec/thread:   80952 lr:  0.006033 avg.loss:  4.065045 ETA:   0h 0m26s 88.7% words/sec/thread:   80805 lr:  0.005647 avg.loss:  4.053253 ETA:   0h 0m24s 88.8% words/sec/thread:   80790 lr:  0.005609 avg.loss:  4.052166 ETA:   0h 0m24s 94.0% words/sec/thread:   80374 lr:  0.003003 avg.loss:  3.889126 ETA:   0h 0m13s 95.8% words/sec/thread:   80301 lr:  0.002098 avg.loss:  3.816962 ETA:   0h 0m 9s 95.9% words/sec/thread:   80295 lr:  0.002056 avg.loss:  3.813586 ETA:   0h 0m 9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 = 0.6840981856990395\n",
      "Macro F1 = 0.7030560723927989\n"
     ]
    }
   ],
   "source": [
    "target = 'data'\n",
    "pred, label = FastText(target)\n",
    "micro, macro = get_accuracy(pred, label)\n",
    "print('Micro F1 = ' + str(micro))\n",
    "print('Macro F1 = ' + str(macro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1673555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
